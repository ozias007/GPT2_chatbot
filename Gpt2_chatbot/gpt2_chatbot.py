# -*- coding: utf-8 -*-
"""Gpt2_Chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12GjtE94ngGgEcd3Hh87fveNp36U1hYmR
"""

!pip install torch
!pip install transformers

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load pre-trained model and tokenizer
model_name = "gpt2-large"  # You can also try "gpt2-medium" or "gpt2-large"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

def chatbot_conversation():
    print("Chatbot: Hi there! I'm your chatbot. Let's have a conversation. Type 'exit' to end the conversation.")
    chat_history = []

    while True:
        user_input = input("You: ")

        if user_input.lower() in ["exit", "quit"]:
            print("Chatbot: Goodbye!")
            break

        chat_history.append(user_input)

        input_text = "\n".join(chat_history)
        input_ids = tokenizer.encode(input_text, return_tensors="pt")

        # Generate response from the chatbot
        with torch.no_grad():
            response_ids = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)

        chatbot_response = tokenizer.decode(response_ids[0], skip_special_tokens=True)
        chat_history.append(chatbot_response)

        print(f"Chatbot: {chatbot_response}")
        chatbot_question = input("Chatbot: What else would you like to know?\nYou: ")

        chat_history.append(chatbot_question)

chatbot_conversation()